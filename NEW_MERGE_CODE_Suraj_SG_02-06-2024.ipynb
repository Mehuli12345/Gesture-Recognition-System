{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14770a60",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyautogui\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygetwindow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgw\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks Audio API.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[0;32m     21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifier\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\audio_classifier.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_options_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_audio_task_api\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_data \u001b[38;5;28;01mas\u001b[39;00m audio_data_module\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_result \u001b[38;5;28;01mas\u001b[39;00m classification_result_module\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\core\\base_audio_task_api.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_record\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptional_dependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     27\u001b[0m _TaskRunner \u001b[38;5;241m=\u001b[39m task_runner_module\u001b[38;5;241m.\u001b[39mTaskRunner\n\u001b[0;32m     28\u001b[0m _Packet \u001b[38;5;241m=\u001b[39m packet_module\u001b[38;5;241m.\u001b[39mPacket\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\core\\optional_dependencies.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TensorFlow isn't a dependency of mediapipe pip package. It's only\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# required in the API docgen pipeline so we'll ignore it if tensorflow is not\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# installed.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;66;03m# Replace the real doc_controls.do_not_generate_docs with an no-op\u001b[39;00m\n\u001b[0;32m     23\u001b[0m   doc_controls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_handle_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[0;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m   ,\n\u001b[0;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     19\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x64\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;130;01m\\x32\u001b[39;00m\u001b[38;5;124m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124munknown_rank\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x08\u001b[39;00m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x44\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124msize\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;130;01m\\x87\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;124morg.tensorflow.frameworkB\u001b[39m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;124mTensorShapeProtosP\u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[38;5;130;01m\\xf8\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     34\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[0;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     45\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     46\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     47\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR),\n\u001b[0;32m     50\u001b[0m   ],\n\u001b[0;32m     51\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     52\u001b[0m   ],\n\u001b[0;32m     53\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     54\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     55\u001b[0m   ],\n\u001b[0;32m     56\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     60\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     61\u001b[0m   ],\n\u001b[0;32m     62\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m149\u001b[39m,\n\u001b[0;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[38;5;241m.\u001b[39mcontaining_type \u001b[38;5;241m=\u001b[39m _TENSORSHAPEPROTO\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\google\\protobuf\\descriptor.py:553\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[0;32m    548\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[0;32m    549\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    552\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Create an intermediate model excluding the last layer\n",
    "intermediate_model = Sequential()\n",
    "for layer in model.layers[:-1]:\n",
    "    intermediate_model.add(layer)\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            msg = actions[np.argmax(res)] if np.max(res) >= 0.60 else \"None\"\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301922ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d738944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4fb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c84f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8f026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b023bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Up\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "None\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "None\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Backspace\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ctrl_A\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Backspace\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n"
     ]
    }
   ],
   "source": [
    "#remove intermediate layer\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            msg = actions[np.argmax(res)] if np.max(res) >= 0.60 else \"None\"\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0dee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0fc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908060a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3aee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48c73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d07f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 496ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n"
     ]
    }
   ],
   "source": [
    "#logic : smooth\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2f274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8663feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Obtaining dependency information for pyttsx3 from https://files.pythonhosted.org/packages/33/9a/de4781245f5ad966646fd276259ef7cfd400ba3cf7d5db7c0d5aab310c20/pyttsx3-2.90-py3-none-any.whl.metadata\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Obtaining dependency information for comtypes from https://files.pythonhosted.org/packages/f5/c0/14dae7492649d7b41cc4f1dd392dd7bb2bc46a07f099f1d2cf4d8dff03e5/comtypes-1.4.4-py3-none-any.whl.metadata\n",
      "  Downloading comtypes-1.4.4-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Obtaining dependency information for pypiwin32 from https://files.pythonhosted.org/packages/d0/1b/2f292bbd742e369a100c91faa0483172cd91a1a422a6692055ac920946c5/pypiwin32-223-py3-none-any.whl.metadata\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Downloading comtypes-1.4.4-py3-none-any.whl (210 kB)\n",
      "   ---------------------------------------- 0.0/210.6 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 81.9/210.6 kB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 174.1/210.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 210.6/210.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.4.4 pypiwin32-223 pyttsx3-2.90\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba649cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like\n",
      "Dislike\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Like\n",
      "Dislike\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "            engine.say(msg)  # Add this line for voice output\n",
    "            engine.runAndWait()  # Add this line to wait until the speech is finished\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        engine.say(msg)  # Add this line for voice output\n",
    "                        engine.runAndWait()  # Add this line to wait until the speech is finished\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "    engine.say(stat_msgs.get(msg, \"None\"))  # Add this line for voice output\n",
    "    engine.runAndWait()  # Add this line to wait until the speech is finished\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345f15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c57fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f58c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 264ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Up\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')  # volume up\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')  # volume down\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('p')  # for previous song\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('n')  # for next song\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')  # pause/play\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.press('backspace')\n",
    "    elif action == \"Tab\":\n",
    "        pyautogui.press('tab')\n",
    "    elif action == \"Ctrl_A\":\n",
    "        pyautogui.hotkey('ctrl', 'a')  # select all\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5f7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9a72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f709a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Like\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Up\n"
     ]
    }
   ],
   "source": [
    "#pause logic\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40860b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c2d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3964d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7460368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ec3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ae752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e03a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a5f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55aae07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "1/1 [==============================] - 1s 607ms/step\n",
      "Prediction: Swipe Up, Confidence: 0.9932403564453125, Smoothed Prediction: Swipe Up\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: Swipe Up, Confidence: 0.9146641492843628, Smoothed Prediction: Swipe Up\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Prediction: Swipe Right, Confidence: 0.49095842242240906, Smoothed Prediction: Swipe Right\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Prediction: Swipe Right, Confidence: 0.9997652173042297, Smoothed Prediction: Swipe Right\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Prediction: Swipe Left, Confidence: 0.46716180443763733, Smoothed Prediction: Swipe Left\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n",
      "Like detected. Clicking the Like button.\n"
     ]
    }
   ],
   "source": [
    "#impliment Like -- Test1\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Coordinates for YouTube Like button (to be adjusted as per actual position on your screen)\n",
    "like_button_coords = (100, 200)  # Example coordinates; replace with actual coordinates\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.75:  # Increased confidence threshold\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(f\"Prediction: {actions[prediction]}, Confidence: {confidence}, Smoothed Prediction: {msg}\")\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    if msg == \"like\":\n",
    "        print(\"Like detected. Clicking the Like button.\")\n",
    "        pyautogui.moveTo(*like_button_coords)\n",
    "        pyautogui.click()\n",
    "    else:\n",
    "        print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381bbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93e73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b61e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a0821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878e634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hotkey Impliment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "810cf9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 264ms/step\n",
      "Swipe Up\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ctrl_A\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Ctrl_A\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Backspace\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Like\n",
      "Like\n",
      "Like\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "Dislike\n",
      "Dislike\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ctrl_A\n",
      "Love\n",
      "Request\n",
      "Request\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ctrl_A\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5'        \n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):  \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    if action == \"Swipe Up\":\n",
    "#         pyautogui.hotkey('volumeup')  # volume up\n",
    "#         pyautogui.scroll(10)  # screen scroll upward\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')  # volume down\n",
    "#         pyautogui.scroll(-10)  # screen scroll downward\n",
    "    elif action == \"Swipe Right\":\n",
    "#         pyautogui.hotkey('p')  # for previous song\n",
    "#         pyautogui.hotkey('right') \n",
    "    elif action == \"Swipe Left\": \n",
    "           pyautogui.hotkey('left')   \n",
    "    elif action == \"Enter\":\n",
    "#         pyautogui.press('space')  # pause/play\n",
    "    elif action == \"Backspace\":\n",
    "#         pyautogui.press('backspace')\n",
    "    elif action == \"Tab\":\n",
    "#         pyautogui.press('tab')\n",
    "    elif action == \"Ctrl_A\":\n",
    "#         pyautogui.hotkey('ctrl', 'a')  # select all\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8b9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0df3605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "Like\n",
      "Executing action: like\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "Dislike\n",
      "Executing action: dislike\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ctrl_A\n",
      "Executing action: Ctrl_A\n",
      "Love\n",
      "Executing action: love\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time  # Import time for adding delay\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return 0, msg\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):  \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "    if action == \"Swipe Up\":\n",
    "#         pyautogui.scroll(50)\n",
    "        engine.say(\"Page Scroll Up\")\n",
    "    elif action == \"Swipe Down\":\n",
    "#         pyautogui.scroll(-50)\n",
    "        engine.say(\"Page Scroll Down\")\n",
    "    elif action == \"Swipe Right\":\n",
    "#         pyautogui.hotkey('j')\n",
    "        engine.say(\"Next Post\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        # Uncomment and add functionality as needed\n",
    "#         pyautogui.hotkey('k')\n",
    "        engine.say(\"Previous Post\")\n",
    "#     elif action == \"Enter\":\n",
    "        # pyautogui.press('space')  # Example: pause/play\n",
    "#         engine.say(\"Press Enter\")\n",
    "    elif action == \"Backspace\":\n",
    "#         pyautogui.hotkey('esc')\n",
    "        engine.say(\"go to back\")\n",
    "#     elif action == \"Tab\":\n",
    "#         pyautogui.press('tab')\n",
    "#         engine.say(\"Select Button\")\n",
    "#     elif action == \"Ctrl_A\":\n",
    "#         pyautogui.hotkey('ctrl', 'a')\n",
    "#         engine.say(\"Select all\")\n",
    "\n",
    "\n",
    "# # Implement PyAutoGUI actions based on gestures based on YT & VLC\n",
    "# def execute_action(action):\n",
    "#     print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "#     if action == \"Swipe Up\":\n",
    "#         pyautogui.scroll(50)\n",
    "#         engine.say(\"Page Scroll Up\")\n",
    "#     elif action == \"Swipe Down\":\n",
    "#         pyautogui.scroll(-50)\n",
    "#         engine.say(\"Page Scroll Down\")\n",
    "# #     elif action == \"Swipe Right\":\n",
    "# #         pyautogui.hotkey('n')\n",
    "# #         engine.say(\"Next Item\")\n",
    "# #     elif action == \"Swipe Left\":\n",
    "# #         # Uncomment and add functionality as needed\n",
    "# #         # pyautogui.hotkey('p')\n",
    "# #         engine.say(\"Previous Item\")\n",
    "# #     elif action == \"Enter\":\n",
    "#         # pyautogui.press('space')  # Example: pause/play\n",
    "# #         engine.say(\"Press Enter\")\n",
    "#     elif action == \"Backspace\":\n",
    "#         pyautogui.hotkey('esc')\n",
    "#         engine.say(\"go to back\")\n",
    "# #     elif action == \"Tab\":\n",
    "# #         pyautogui.press('tab')\n",
    "# #         engine.say(\"Select Button\")\n",
    "# #     elif action == \"Ctrl_A\":\n",
    "# #         pyautogui.hotkey('ctrl', 'a')\n",
    "# #         engine.say(\"Select all\")\n",
    "\n",
    "\n",
    "# # Implement PyAutoGUI actions based on gestures\n",
    "# def execute_action(action):\n",
    "#     print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "#     if action == \"Swipe Up\":\n",
    "# #         pyautogui.scroll(50)\n",
    "#         engine.say(\"Swipe Up\")\n",
    "#     elif action == \"Swipe Down\":\n",
    "# #         pyautogui.scroll(-50)\n",
    "#         engine.say(\"Swipe Down\")\n",
    "#     elif action == \"Swipe Right\":\n",
    "# #         pyautogui.hotkey('n')\n",
    "#         engine.say(\"Swipe Righ\")\n",
    "#     elif action == \"Swipe Left\":\n",
    "# #         # Uncomment and add functionality as needed\n",
    "# #         # pyautogui.hotkey('p')\n",
    "#         engine.say(\"Swipe Left\")\n",
    "#     elif action == \"Enter\":\n",
    "#         # pyautogui.press('space')  # Example: pause/play\n",
    "#         engine.say(\"Enter\")\n",
    "#     elif action == \"Backspace\":\n",
    "# #         pyautogui.hotkey('esc')\n",
    "#         engine.say(\"Backspace\")\n",
    "#     elif action == \"Tab\":\n",
    "# #         pyautogui.press('tab')\n",
    "#         engine.say(\"Tab\")\n",
    "#     elif action == \"Ctrl_A\":\n",
    "# #         pyautogui.hotkey('ctrl', 'a')\n",
    "#         engine.say(\"Select all\")\n",
    "    \n",
    "    engine.runAndWait()\n",
    "    # time.sleep(0.8)  # Optionally add a delay after each gesture\n",
    "    \n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e044a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745ac48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef917c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56636a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########    START     ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef1e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae420ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea158c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Facebook ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1209a360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "Ctrl_A\n",
      "Executing action: Ctrl_A\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "Like\n",
      "Executing action: like\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "Executing action: \n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ctrl_A\n",
      "Executing action: Ctrl_A\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "Executing action: \n",
      "Executing action: \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time  # Import time for adding delay\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return 0, msg\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):  \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.scroll(50)\n",
    "        engine.say(\"Page Scroll Up\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.scroll(-50)\n",
    "        engine.say(\"Page Scroll Down\")\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('j')\n",
    "        engine.say(\"Next Post\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('k')\n",
    "        engine.say(\"Previous Post\")\n",
    "    elif action == \"like\":\n",
    "        pyautogui.hotkey('l')\n",
    "        engine.say(\"Like The Post\") \n",
    "    elif action == \"dislike\":\n",
    "        pyautogui.hotkey('l')\n",
    "        engine.say(\"DisLike The Post\")\n",
    "#     elif action == \"Enter\":\n",
    "        # pyautogui.press('space')  # Example: pause/play\n",
    "#         engine.say(\"Press Enter\")\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.hotkey('esc')\n",
    "        engine.say(\"go to back\")\n",
    "#     elif action == \"Tab\":\n",
    "#         pyautogui.press('tab')\n",
    "#         engine.say(\"Select Button\")\n",
    "#     elif action == \"Ctrl_A\":\n",
    "#         pyautogui.hotkey('ctrl', 'a')\n",
    "#         engine.say(\"Select all\")\n",
    "\n",
    "\n",
    "   \n",
    "    engine.runAndWait()\n",
    "    # time.sleep(0.8)  # Optionally add a delay after each gesture\n",
    "    \n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b0a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dae2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##gmail##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53e2058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "Love\n",
      "Executing action: love\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "Love\n",
      "Executing action: love\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "click occurred\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "click occurred\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time  # Import time for adding delay\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set text-to-speech voice to 'hi-IN'\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if 'hi-IN' in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return 0, msg\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):  \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('k')\n",
    "        engine.say(\"Previous Message\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('j')\n",
    "        engine.say(\"Next Message\")\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('g', 'n')\n",
    "        engine.say(\"Go to Next Page\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('g', 'p')\n",
    "        engine.say(\"Go to Previous Page\")\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('enter')  # Example: pause/play\n",
    "        engine.say(\"Open Message\")\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.hotkey('g', 'i')\n",
    "        engine.say(\"Go back\")\n",
    "#     elif action == \"Tab\":\n",
    "#         pyautogui.press('tab')\n",
    "#         engine.say(\"Select Button\")\n",
    "#     elif action == \"Ctrl_A\":\n",
    "#         pyautogui.hotkey('ctrl', 'a')\n",
    "#         engine.say(\"Select all\")\n",
    "\n",
    "\n",
    "   \n",
    "    engine.runAndWait()\n",
    "    # time.sleep(0.8)  # Optionally add a delay after each gesture\n",
    "    \n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf68139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d883c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 430ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "Like\n",
      "Executing action: like\n",
      "like\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "Dislike\n",
      "Executing action: dislike\n",
      "dislike\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "Executing action: \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set text-to-speech voice to 'hi-IN'\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if 'hi-IN' in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return 0, msg\n",
    "\n",
    "def find_button_location(button_location):\n",
    "    if button_location is not None:\n",
    "        button_center = pyautogui.center(button_location)\n",
    "        pyautogui.moveTo(button_center)\n",
    "        pyautogui.click()\n",
    "    else:\n",
    "        print(\"Button not found on the screen.\")\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')\n",
    "        engine.say(\"Volume Increase\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')\n",
    "        engine.say(\"Volume Decrease\")\n",
    "    elif action == \"Swipe Right\":\n",
    "#         pyautogui.hotkey('l')\n",
    "        pyautogui.hotkey('right')\n",
    "        engine.say(\"5 seconds forward\")\n",
    "    elif action == \"Swipe Left\":\n",
    "#         pyautogui.hotkey('j')\n",
    "        pyautogui.hotkey('left')\n",
    "        engine.say(\"5 seconds backward\")\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')\n",
    "        engine.say(\"Play or Pause\")\n",
    "    elif action == \"Backspace\":\n",
    "        # Uncomment and adjust as necessary\n",
    "        # pyautogui.hotkey('g', 'i')\n",
    "        # engine.say(\"Go back\")\n",
    "        pass\n",
    "    elif action == 'like':\n",
    "        print('like')\n",
    "        try:\n",
    "            like_location = pyautogui.locateOnScreen('like_1.png', confidence=0.8)\n",
    "            if like_location:\n",
    "                find_button_location(like_location)\n",
    "            else:\n",
    "                like_location = pyautogui.locateOnScreen('like_button1.png', confidence=0.8)\n",
    "                if like_location:\n",
    "                    find_button_location(like_location)\n",
    "                else:\n",
    "                    like_location = pyautogui.locateOnScreen('Like_dark.png', confidence=0.8)\n",
    "                    if like_location:\n",
    "                        find_button_location(like_location)\n",
    "                    else:\n",
    "                        print(\"Like button not found on the screen.\")\n",
    "        except pyautogui.ImageNotFoundException:\n",
    "            print(\"Button not found on the screen.\")\n",
    "    elif action == 'dislike':\n",
    "        print('dislike')\n",
    "        try:\n",
    "            dislike_location = pyautogui.locateOnScreen('dis_like.png', confidence=0.8)\n",
    "            if dislike_location:\n",
    "                find_button_location(dislike_location)\n",
    "            else:\n",
    "                dislike_location = pyautogui.locateOnScreen('dis_like2.png', confidence=0.8)\n",
    "                if dislike_location:\n",
    "                    find_button_location(dislike_location)\n",
    "                else:\n",
    "                    dislike_location = pyautogui.locateOnScreen('Dis_like_dark.png', confidence=0.8)\n",
    "                    if dislike_location:\n",
    "                        find_button_location(dislike_location)\n",
    "                    else:\n",
    "                        print(\"Dislike button not found on the screen.\")\n",
    "        except pyautogui.ImageNotFoundException:\n",
    "            print(\"Button not found on the screen.\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515638d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a04736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2d8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########END###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43edd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc38f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'temp.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 299\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fun1(keys, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 299\u001b[0m         execute_action(msg)\n\u001b[0;32m    300\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xff\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[6], line 258\u001b[0m, in \u001b[0;36mexecute_action\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m    255\u001b[0m     pyautogui\u001b[38;5;241m.\u001b[39mhotkey(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    256\u001b[0m     tts \u001b[38;5;241m=\u001b[39m gTTS(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGo back\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, tld\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mco.in\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 258\u001b[0m tts\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    259\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    260\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mplay()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umidd\\lib\\site-packages\\gtts\\tts.py:324\u001b[0m, in \u001b[0;36mgTTS.save\u001b[1;34m(self, savefile)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, savefile):\n\u001b[0;32m    315\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do the TTS API request and write result to file.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    322\u001b[0m \n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msavefile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_to_fp(f)\n\u001b[0;32m    326\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, savefile)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'temp.mp3'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Initialize Pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return 0, msg\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        tts = gTTS(action, lang='en', tld='co.in')\n",
    "        tts.save(\"temp.mp3\")\n",
    "        pygame.mixer.music.load(\"temp.mp3\")\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            time.sleep(0.1)\n",
    "        os.remove(\"temp.mp3\")\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")\n",
    "    tts = gTTS(\"Unknown Action\", lang='en', tld='co.in')\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('k')\n",
    "        tts = gTTS(\"Previous Message\", lang='en', tld='co.in')\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('j')\n",
    "        tts = gTTS(\"Next Message\", lang='en', tld='co.in')\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('g', 'n')\n",
    "        tts = gTTS(\"Go to Next Page\", lang='en', tld='co.in')\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('g', 'p')\n",
    "        tts = gTTS(\"Go to Previous Page\", lang='en', tld='co.in')\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('enter')\n",
    "        tts = gTTS(\"Open Message\", lang='en', tld='co.in')\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.hotkey('g', 'i')\n",
    "        tts = gTTS(\"Go back\", lang='en', tld='co.in')\n",
    "\n",
    "    tts.save(\"temp.mp3\")\n",
    "    pygame.mixer.music.load(\"temp.mp3\")\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "    time.sleep(1)  # Small delay to ensure file is not being used\n",
    "    os.remove(\"temp.mp3\")\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bcfe1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba209c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ec85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ad5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371be74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Test COde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efffdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc0f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 5 seconds to switch to the YouTube video window...\n",
      "The 'L' key was pressed.\n"
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "\n",
    "# Add a delay to allow the user to switch to the YouTube video\n",
    "print(\"You have 5 seconds to switch to the YouTube video window...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Simulate pressing the 'L' key\n",
    "pyautogui.hotkey('l')\n",
    "print(\"The 'L' key was pressed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2003708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keyboard\n",
      "  Obtaining dependency information for keyboard from https://files.pythonhosted.org/packages/55/88/287159903c5b3fc6d47b651c7ab65a54dcf9c9916de546188a7f62870d6d/keyboard-0.13.5-py3-none-any.whl.metadata\n",
      "  Downloading keyboard-0.13.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Downloading keyboard-0.13.5-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.1 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 30.7/58.1 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 58.1/58.1 kB 613.6 kB/s eta 0:00:00\n",
      "Installing collected packages: keyboard\n",
      "Successfully installed keyboard-0.13.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd345106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2821199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "def like_video():\n",
    "    # Locate the 'Like' button on the screen using image recognition\n",
    "    try:\n",
    "        like_button_location = pyautogui.locateCenterOnScreen('like_button.png', confidence=0.5)\n",
    "        if like_button_location:\n",
    "            pyautogui.click(like_button_location)\n",
    "            print(\"Clicked the 'Like' button\")\n",
    "        else:\n",
    "            print(\"Could not find the 'Like' button\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Keyboard event listener\n",
    "keyboard.add_hotkey('a', like_video)\n",
    "\n",
    "# Keep the script running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)  # Adjust if needed\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb64b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b7d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d64846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8bfca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ff44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice:\n",
      " - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0\n",
      " - Name: Microsoft David Desktop - English (United States)\n",
      " - Languages: []\n",
      " - Gender: None\n",
      " - Age: None\n",
      "\n",
      "\n",
      "Voice:\n",
      " - ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\n",
      " - Name: Microsoft Zira Desktop - English (United States)\n",
      " - Languages: []\n",
      " - Gender: None\n",
      " - Age: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "for voice in voices:\n",
    "    print(\"Voice:\")\n",
    "    print(\" - ID: %s\" % voice.id)\n",
    "    print(\" - Name: %s\" % voice.name)\n",
    "    print(\" - Languages: %s\" % voice.languages)\n",
    "    print(\" - Gender: %s\" % voice.gender)\n",
    "    print(\" - Age: %s\" % voice.age)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f34fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice:\n",
      " - Name: Microsoft David Desktop - English (United States)\n",
      " - Languages: []\n",
      "\n",
      "\n",
      "Voice:\n",
      " - Name: Microsoft Zira Desktop - English (United States)\n",
      " - Languages: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "for voice in voices:\n",
    "    print(\"Voice:\")\n",
    "    print(\" - Name: %s\" % voice.name)\n",
    "    print(\" - Languages: %s\" % voice.languages)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ade548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217de03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f4e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set properties before adding voice\n",
    "engine.setProperty('rate', 150)    # Speed percent (can go over 100)\n",
    "engine.setProperty('volume', 0.9)  # Volume 0-1\n",
    "\n",
    "# Specify a voice with Indian English accent\n",
    "engine.setProperty('voice', 'HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-IN_KALPANA_11.0')\n",
    "\n",
    "# Test the voice\n",
    "engine.say(\"Hello, how are you?\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "457b7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "tts = gTTS('Backspace - moving the right-hand fist from right to left with the thumb pointing towards the left.', lang='en', tld='co.in')\n",
    "tts.save('hello1.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f3f3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gtts import gTTS\n",
    "# from io import BytesIO\n",
    "\n",
    "# mp3_fp = BytesIO()\n",
    "# tts = gTTS('Backspace - moving the right-hand fist from right to left with the thumb pointing towards the left.', lang='en',tld='co.in')\n",
    "# tts.write_to_fp(mp3_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd8591dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gtts.tts.gTTS at 0x1c11682e7d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af569c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gtts in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: playsound in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from gtts) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from gtts) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from requests<3,>=2.27->gtts) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (from requests<3,>=2.27->gtts) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install gtts playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca1f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages (2.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e46c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
