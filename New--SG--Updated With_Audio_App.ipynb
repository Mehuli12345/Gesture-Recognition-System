{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########    START     ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bdc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Facebook ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bbfa18",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyautogui\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygetwindow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgw\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks Audio API.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[0;32m     21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifier\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\audio_classifier.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_options_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_audio_task_api\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_data \u001b[38;5;28;01mas\u001b[39;00m audio_data_module\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_result \u001b[38;5;28;01mas\u001b[39;00m classification_result_module\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\core\\base_audio_task_api.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_record\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptional_dependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     27\u001b[0m _TaskRunner \u001b[38;5;241m=\u001b[39m task_runner_module\u001b[38;5;241m.\u001b[39mTaskRunner\n\u001b[0;32m     28\u001b[0m _Packet \u001b[38;5;241m=\u001b[39m packet_module\u001b[38;5;241m.\u001b[39mPacket\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\mediapipe\\tasks\\python\\core\\optional_dependencies.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TensorFlow isn't a dependency of mediapipe pip package. It's only\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# required in the API docgen pipeline so we'll ignore it if tensorflow is not\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# installed.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;66;03m# Replace the real doc_controls.do_not_generate_docs with an no-op\u001b[39;00m\n\u001b[0;32m     23\u001b[0m   doc_controls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_handle_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[0;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m   ,\n\u001b[0;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     19\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x64\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;130;01m\\x32\u001b[39;00m\u001b[38;5;124m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124munknown_rank\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x08\u001b[39;00m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x44\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124msize\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;130;01m\\x87\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;124morg.tensorflow.frameworkB\u001b[39m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;124mTensorShapeProtosP\u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[38;5;130;01m\\xf8\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     34\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[0;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     45\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     46\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     47\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR),\n\u001b[0;32m     50\u001b[0m   ],\n\u001b[0;32m     51\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     52\u001b[0m   ],\n\u001b[0;32m     53\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     54\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     55\u001b[0m   ],\n\u001b[0;32m     56\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     60\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     61\u001b[0m   ],\n\u001b[0;32m     62\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m149\u001b[39m,\n\u001b[0;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[38;5;241m.\u001b[39mcontaining_type \u001b[38;5;241m=\u001b[39m _TENSORSHAPEPROTO\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umproject\\lib\\site-packages\\google\\protobuf\\descriptor.py:553\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[0;32m    548\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[0;32m    549\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    552\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time  # Import time for adding delay\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return 0, msg\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):  \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.scroll(50)\n",
    "        engine.say(\"Page Scroll Up\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.scroll(-50)\n",
    "        engine.say(\"Page Scroll Down\")\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('j')\n",
    "        engine.say(\"Next Post\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('k')\n",
    "        engine.say(\"Previous Post\")\n",
    "    elif action == \"like\":\n",
    "        pyautogui.hotkey('l')\n",
    "        engine.say(\"Like The Post\") \n",
    "    elif action == \"dislike\":\n",
    "        pyautogui.hotkey('l')\n",
    "        engine.say(\"DisLike The Post\")\n",
    "#     elif action == \"Enter\":\n",
    "        # pyautogui.press('space')  # Example: pause/play\n",
    "#         engine.say(\"Press Enter\")\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.hotkey('esc')\n",
    "        engine.say(\"go to back\")\n",
    "#     elif action == \"Tab\":\n",
    "#         pyautogui.press('tab')\n",
    "#         engine.say(\"Select Button\")\n",
    "#     elif action == \"Ctrl_A\":\n",
    "#         pyautogui.hotkey('ctrl', 'a')\n",
    "#         engine.say(\"Select all\")\n",
    "\n",
    "\n",
    "   \n",
    "    engine.runAndWait()\n",
    "    # time.sleep(0.8)  # Optionally add a delay after each gesture\n",
    "    \n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844a3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2191bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##gmail##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293998ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4e130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Tab\n",
      "Executing action: Tab\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Backspace\n",
      "Executing action: Backspace\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time  # Import time for adding delay\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set text-to-speech voice to 'hi-IN'\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if 'hi-IN' in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):  \n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")  # Add this line for debugging\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('k')\n",
    "        engine.say(\"Previous Message\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('j')\n",
    "        engine.say(\"Next Message\")\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('g', 'n')\n",
    "        engine.say(\"Go to Next Page\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('g', 'p')\n",
    "        engine.say(\"Go to Previous Page\")\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('enter')  # Example: pause/play\n",
    "        engine.say(\"Open Message\")\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.hotkey('g', 'i')\n",
    "        engine.say(\"Go back\")\n",
    "    elif action == \"Tab\":\n",
    "        pyautogui.press('tab')\n",
    "        engine.say(\"Select Button\")\n",
    "    elif action == \"Ctrl_A\":\n",
    "        pyautogui.hotkey('ctrl', 'a')\n",
    "        engine.say(\"Select all\")\n",
    "\n",
    "    engine.runAndWait()\n",
    "    # time.sleep(0.8)  # Optionally add a delay after each gesture\n",
    "    \n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf3bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc03e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "##youtube & VLC ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c5593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3425fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing action: \n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Swipe Up\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Swipe Down\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Swipe Left\n",
      "Executing action: Swipe Left\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "Dislike\n",
      "Executing action: dislike\n",
      "dislike\n",
      "Dislike button not found on the screen.\n",
      "Dislike\n",
      "Executing action: dislike\n",
      "dislike\n",
      "Dislike button not found on the screen.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Enter\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ctrl_A\n",
      "Executing action: Ctrl_A\n",
      "Dislike\n",
      "Executing action: dislike\n",
      "dislike\n",
      "Dislike button not found on the screen.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Swipe Right\n",
      "Executing action: Swipe Right\n",
      "Executing action: \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set text-to-speech voice to 'hi-IN'\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if 'hi-IN' in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')\n",
    "        engine.say(\"Volume Increase\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')\n",
    "        engine.say(\"Volume Decrease\")\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('right')\n",
    "        engine.say(\"5 seconds forward\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('left')\n",
    "        engine.say(\"5 seconds backward\")\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')\n",
    "        engine.say(\"Play or Pause\")\n",
    "    elif action == \"Backspace\":\n",
    "        pass  # No action for backspace\n",
    "    elif action == 'like':\n",
    "        print('like')\n",
    "        try:\n",
    "            like_location = pyautogui.locateOnScreen('like_1.png', confidence=0.8)\n",
    "            if like_location:\n",
    "                pyautogui.click(like_location)\n",
    "            else:\n",
    "                like_location = pyautogui.locateOnScreen('like_button1.png', confidence=0.8)\n",
    "                if like_location:\n",
    "                    pyautogui.click(like_location)\n",
    "                else:\n",
    "                    like_location = pyautogui.locateOnScreen('Like_dark.png', confidence=0.8)\n",
    "                    if like_location:\n",
    "                        pyautogui.click(like_location)\n",
    "                    else:\n",
    "                        print(\"Like button not found on the screen.\")\n",
    "        except pyautogui.ImageNotFoundException:\n",
    "            print(\"Button not found on the screen.\")\n",
    "    elif action == 'dislike':\n",
    "        print('dislike')\n",
    "        try:\n",
    "            dislike_location = pyautogui.locateOnScreen('dis_like.png', confidence=0.8)\n",
    "            if dislike_location:\n",
    "                pyautogui.click(dislike_location)\n",
    "            else:\n",
    "                dislike_location = pyautogui.locateOnScreen('dis_like2.png', confidence=0.8)\n",
    "                if dislike_location:\n",
    "                    pyautogui.click(dislike_location)\n",
    "                else:\n",
    "                    dislike_location = pyautogui.locateOnScreen('Dis_like_dark.png', confidence=0.8)\n",
    "                    if dislike_location:\n",
    "                        pyautogui.click(dislike_location)\n",
    "                    else:\n",
    "                        print(\"Dislike button not found on the screen.\")\n",
    "        except pyautogui.ImageNotFoundException:\n",
    "            print(\"Button not found on the screen.\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b87c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae27c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b7758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13cc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c61d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98699bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing action: \n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Executing action: Swipe Up\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Executing action: Enter\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Executing action: Swipe Down\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Swipe Right\n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Swipe Right\n",
      "Executing action: \n",
      "Executing action: \n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Executing action: Swipe Right\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Backspace\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Backspace\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Executing action: Backspace\n",
      "Executing action: \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set text-to-speech voice to 'hi-IN'\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    if 'hi-IN' in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=clr))\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    action_time = time.time()\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            # Implement debounce to prevent multiple actions\n",
    "            if time.time() - action_time > 0.5:\n",
    "                action_time = time.time()\n",
    "                cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    action = stat_msgs.get(msg, \"None\")\n",
    "    if action != \"None\":\n",
    "        print(action)\n",
    "        engine.say(action)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(gest[0])  # Append the index of the gesture\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Current Gesture : \" + statuses[gest[0]], (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    most_common_gesture_index = Counter(res).most_common(1)[0][0]\n",
    "    return res, ret_keypoints, statuses[most_common_gesture_index]\n",
    "\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    print(f\"Executing action: {action}\")\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')\n",
    "        engine.say(\"Volume Increase\")\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')\n",
    "        engine.say(\"Volume Decrease\")\n",
    "    elif action == \"Swipe Right\":\n",
    "        pyautogui.hotkey('right')\n",
    "        engine.say(\"5 seconds forward\")\n",
    "    elif action == \"Swipe Left\":\n",
    "        pyautogui.hotkey('left')\n",
    "        engine.say(\"5 seconds backward\")\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')\n",
    "        engine.say(\"Play or Pause\")\n",
    "    elif action == \"Backspace\":\n",
    "        pass  # No action for backspace\n",
    "    elif action == 'like':\n",
    "        print('Attempting to like...')\n",
    "        try:\n",
    "            for like_img in ['like_1.png', 'like_button1.png', 'Like_dark.png']:\n",
    "                like_location = pyautogui.locateOnScreen(like_img, confidence=0.8)\n",
    "                if like_location:\n",
    "                    pyautogui.click(like_location)\n",
    "                    print('Liked successfully')\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Like button not found on the screen.\")\n",
    "        except pyautogui.ImageNotFoundException:\n",
    "            print(\"Like button images not found on the screen.\")\n",
    "    elif action == 'dislike':\n",
    "        print('Attempting to dislike...')\n",
    "        try:\n",
    "            for dislike_img in ['dis_like.png', 'dis_like2.png', 'Dis_like_dark.png']:\n",
    "                dislike_location = pyautogui.locateOnScreen(dislike_img, confidence=0.8)\n",
    "                if dislike_location:\n",
    "                    pyautogui.click(dislike_location)\n",
    "                    print('Disliked successfully')\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Dislike button not found on the screen.\")\n",
    "        except pyautogui.ImageNotFoundException:\n",
    "            print(\"Dislike button images not found on the screen.\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                execute_action(s_msg)  # Execute static gesture\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "                execute_action(msg)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a26536c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bddfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
