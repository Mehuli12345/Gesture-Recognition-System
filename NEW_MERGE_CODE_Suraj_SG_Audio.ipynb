{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49605c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\souja\\anaconda3\\envs\\umprojec\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\souja\\anaconda3\\envs\\umprojec\\lib\\site-packages (from h5py) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9370ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_model()` using h5 format requires h5py. Could not import h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and weights\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAllRemain-LSTMv2.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllRemain-LSTMv2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load SVM model for static gesture recognition\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umprojec\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    188\u001b[0m         filepath,\n\u001b[0;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\umprojec\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:102\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads a model saved via `save_model_to_hdf5`.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    ValueError: In case of an invalid savefile.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`load_model()` using h5 format requires h5py. Could not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport h5py.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m custom_objects:\n\u001b[0;32m    108\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mImportError\u001b[0m: `load_model()` using h5 format requires h5py. Could not import h5py."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import pyttsx3\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "async def do_map(k):\n",
    "    action_msgs = {\n",
    "        \"Swipe Up\": \"Scrolling up\",\n",
    "        \"Swipe Down\": \"Scrolling down\",\n",
    "        \"Swipe Right\": \"Scrolling right\",\n",
    "        \"Swipe Left\": \"Scrolling left\",\n",
    "        \"Ctrl_A\": \"Select All\",\n",
    "        \"Tab\": \"Tab\",\n",
    "        \"Backspace\": \"Backspace\",\n",
    "        \"Enter\": \"Enter\"\n",
    "    }\n",
    "    return action_msgs.get(actions[k], \"None\")\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            res = model.predict(np.array([sequence]))\n",
    "            prediction = np.argmax(res)\n",
    "            confidence = np.max(res)\n",
    "            if confidence >= 0.60:\n",
    "                prediction_history.append(actions[prediction])\n",
    "            if len(prediction_history) == prediction_history.maxlen:\n",
    "                most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "                msg = most_common_prediction\n",
    "            else:\n",
    "                msg = actions[prediction]\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            print(msg)\n",
    "            engine.say(msg)  # Add this line for voice output\n",
    "            engine.runAndWait()  # Add this line to wait until the speech is finished\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        print(\"click occurred\")\n",
    "                        engine.say(msg)  # Add this line for voice output\n",
    "                        engine.runAndWait()  # Add this line to wait until the speech is finished\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    print(stat_msgs.get(msg, \"None\"))\n",
    "    engine.say(stat_msgs.get(msg, \"None\"))  # Add this line for voice output\n",
    "    engine.runAndWait()  # Add this line to wait until the speech is finished\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "    \n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')  # volume up\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')  # volume down\n",
    "    elif action == \"Swipe Right\":\n",
    "#         pyautogui.hotkey('p')  # for previous song\n",
    "        pyautogui.hotkey('right')\n",
    "    elif action == \"Swipe Left\":\n",
    "#         pyautogui.hotkey('n')  # for next song\n",
    "        pyautogui.hotkey('left')\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')  # pause/play\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.press('backspace')\n",
    "    elif action == \"Tab\":\n",
    "        pyautogui.press('tab')\n",
    "    elif action == \"Ctrl_A\":\n",
    "        pyautogui.hotkey('ctrl', 'a')  # select all\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab963f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9a48ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load pre-trained model and weights\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllRemain-LSTMv2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllRemain-LSTMv2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load SVM model for static gesture recognition\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "#smooth logic 24/06(1)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import threading\n",
    "import pyttsx3\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "def speak(msg):\n",
    "    engine.say(msg)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        threading.Thread(target=speak, args=(msg,)).start()  # Use threading for voice output\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    stat_msg = stat_msgs.get(msg, \"None\")\n",
    "    print(stat_msg)\n",
    "    threading.Thread(target=speak, args=(stat_msg,)).start()  # Use threading for voice output\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "def process_gesture(gesture_seq, prediction_history, msg):\n",
    "    res = model.predict(np.array([gesture_seq]))\n",
    "    prediction = np.argmax(res)\n",
    "    confidence = np.max(res)\n",
    "    if confidence >= 0.60:\n",
    "        prediction_history.append(actions[prediction])\n",
    "    if len(prediction_history) == prediction_history.maxlen:\n",
    "        most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "        msg = most_common_prediction\n",
    "    else:\n",
    "        msg = actions[prediction]\n",
    "    return msg\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            msg = process_gesture(sequence, prediction_history, msg)\n",
    "            threading.Thread(target=speak, args=(msg,)).start()  # Use threading for voice output\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')  # volume up\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')  # volume down\n",
    "    elif action == \"Swipe Right\":\n",
    "#         pyautogui.hotkey('p')  # for previous song\n",
    "        pyautogui.hotkey('right')\n",
    "    elif action == \"Swipe Left\":\n",
    "#         pyautogui.hotkey('n')  # for next song\n",
    "        pyautogui.hotkey('left')\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')  # pause/play\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.press('backspace')\n",
    "    elif action == \"Tab\":\n",
    "        pyautogui.press('tab')\n",
    "    elif action == \"Ctrl_A\":\n",
    "        pyautogui.hotkey('ctrl', 'a')  # select all\n",
    "        \n",
    "        \n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9add6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589af63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ee005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24168a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\umidd\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "#smooth logic 24/06(2)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import pygetwindow as gw\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter, deque\n",
    "import threading\n",
    "import pyttsx3\n",
    "import queue\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "# Disable PyAutoGUI failsafe\n",
    "pyautogui.FAILSAFE = False\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a queue for text-to-speech\n",
    "tts_queue = queue.Queue()\n",
    "\n",
    "# Function to handle text-to-speech requests\n",
    "def tts_worker():\n",
    "    while True:\n",
    "        msg = tts_queue.get()\n",
    "        if msg is None:\n",
    "            break\n",
    "        engine.say(msg)\n",
    "        engine.runAndWait()\n",
    "        tts_queue.task_done()\n",
    "\n",
    "# Start the text-to-speech thread\n",
    "tts_thread = threading.Thread(target=tts_worker)\n",
    "tts_thread.daemon = True\n",
    "tts_thread.start()\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define actions and statuses\n",
    "actions = np.array([\"Swipe Up\", \"Swipe Down\", \"Swipe Left\", \"Swipe Right\", \"Backspace\", \"Tab\", \"Enter\", \"Ctrl_A\"])\n",
    "statuses = [\"like\", \"love\", \"request\", \"victory\", \"dislike\", \"closed_fist\", \"none\"]\n",
    "\n",
    "# Initialize MediaPipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load pre-trained model and weights\n",
    "model = load_model(r'AllRemain-LSTMv2.h5')\n",
    "model.load_weights(r'AllRemain-LSTMv2.h5')\n",
    "\n",
    "# Load SVM model for static gesture recognition\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    smodel = pickle.load(file)\n",
    "\n",
    "# Load feature vectors\n",
    "loaded_list = np.load(r'avg_600_feature_vector1.npy', allow_pickle=True)\n",
    "\n",
    "# Helper functions\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, clr):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=clr))\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def s_extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    res = np.concatenate([lh, rh])\n",
    "    for i in range(len(res)):\n",
    "        if i < 63:\n",
    "            k = 0\n",
    "        else:\n",
    "            k = 63\n",
    "        if i % 3 == 0:\n",
    "            res[i] = res[i] - res[k]\n",
    "        elif i % 3 == 1:\n",
    "            res[i] = res[i] - res[k + 1]\n",
    "        elif i % 3 == 2:\n",
    "            res[i] = res[i] - res[k + 2]\n",
    "    return res\n",
    "\n",
    "def speak(msg):\n",
    "    tts_queue.put(msg)\n",
    "\n",
    "def eucal(p1, p2):\n",
    "    return math.sqrt((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2)\n",
    "\n",
    "def pointer(dis, msg):\n",
    "    first_time = 1\n",
    "    buffer = 5 if dis < 0 else -2\n",
    "    while dis < 0.10 and (first_time == 1 or (results.right_hand_landmarks and results.left_hand_landmarks is None)):\n",
    "        first_time = 0\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        if results.right_hand_landmarks:\n",
    "            l4 = results.right_hand_landmarks.landmark[4]\n",
    "            l12 = results.right_hand_landmarks.landmark[12]\n",
    "            draw_landmarks(image, results, (0, 0, 255))\n",
    "            dis = eucal(l4, l12)\n",
    "            if dis < 0.10:\n",
    "                l1 = results.right_hand_landmarks.landmark[8]\n",
    "                ix = (screen_width + 10) - (screen_width + 10) * l1.x\n",
    "                iy = (screen_height + 10) * l1.y\n",
    "                pyautogui.moveTo(ix, iy)\n",
    "                if buffer > 0:\n",
    "                    return 1, msg\n",
    "        if dis >= 0.10 or results.right_hand_landmarks is None:\n",
    "            if buffer == -2:\n",
    "                val, msg = pointer(-1, msg)\n",
    "                if val == 1:\n",
    "                    dis = 0\n",
    "                    first_time = 1\n",
    "            elif buffer > 0:\n",
    "                cv2.putText(image, \"In rec\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                if results.right_hand_landmarks:\n",
    "                    l11 = results.right_hand_landmarks.landmark[11].y\n",
    "                    l7 = results.right_hand_landmarks.landmark[7].y\n",
    "                    if l11 < l7:\n",
    "                        pyautogui.press('ctrl', presses=5)\n",
    "                        pyautogui.click()\n",
    "                        msg = \"Click\"\n",
    "                        threading.Thread(target=speak, args=(msg,)).start()  # Use threading for voice output\n",
    "                        return 1, msg\n",
    "                buffer -= 1\n",
    "                dis = 0\n",
    "                first_time = 1\n",
    "            else:\n",
    "                return 0, msg\n",
    "        cv2.putText(image, \"Action : Pointer\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            return\n",
    "\n",
    "def map_stat(msg):\n",
    "    stat_msgs = {\n",
    "        \"like\": \"Like\",\n",
    "        \"love\": \"Love\",\n",
    "        \"dislike\": \"Dislike\",\n",
    "        \"request\": \"Request\",\n",
    "        \"victory\": \"Victory\",\n",
    "        \"closed_fist\": \"Fist Closed\",\n",
    "        \"none\": \"None\"\n",
    "    }\n",
    "    stat_msg = stat_msgs.get(msg, \"None\")\n",
    "    print(stat_msg)\n",
    "    speak(stat_msg)  # Use speak function to add the message to the queue\n",
    "\n",
    "def static():\n",
    "    res = []\n",
    "    total = 5\n",
    "    msg = \"\"\n",
    "    ret_keypoints = []\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results, (0, 255, 0))\n",
    "        keypoints = s_extract_keypoints(results)\n",
    "        keypts = extract_keypoints(results)\n",
    "        ret_keypoints.append(keypts)\n",
    "        gest = smodel.predict([keypoints])\n",
    "        res.append(statuses[gest[0]])\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last Gesture : \" + res[-1], (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    frequency = Counter(res)\n",
    "    for element, count in frequency.items():\n",
    "        if count == total:\n",
    "            msg = element\n",
    "            break\n",
    "    if msg and msg not in {\"none\", \"closed_fist\"}:\n",
    "        map_stat(msg)\n",
    "        return 1, ret_keypoints, msg\n",
    "    else:\n",
    "        return 0, ret_keypoints, \"None\"\n",
    "\n",
    "def process_gesture(gesture_seq, prediction_history, msg):\n",
    "    res = model.predict(np.array([gesture_seq]))\n",
    "    prediction = np.argmax(res)\n",
    "    confidence = np.max(res)\n",
    "    if confidence >= 0.60:\n",
    "        prediction_history.append(actions[prediction])\n",
    "    if len(prediction_history) == prediction_history.maxlen:\n",
    "        most_common_prediction = Counter(prediction_history).most_common(1)[0][0]\n",
    "        msg = most_common_prediction\n",
    "    else:\n",
    "        msg = actions[prediction]\n",
    "    return msg\n",
    "\n",
    "async def fun1(initial_sequence, flag):\n",
    "    sequence = initial_sequence if flag else []\n",
    "    msg = \"\"\n",
    "    prediction_history = deque(maxlen=5)\n",
    "    while len(sequence) <= 20:\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        if len(sequence) == 10:\n",
    "            if sum(np.all(seq == 0) for seq in sequence) > 5:\n",
    "                return \"\"\n",
    "            sequence = [np.zeros(126)] * 5 + sequence + [np.zeros(126)] * 5\n",
    "            msg = process_gesture(sequence, prediction_history, msg)\n",
    "            threading.Thread(target=speak, args=(msg,)).start()  # Use threading for voice output\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "        cv2.putText(image, \"Last Gesture : \" + msg, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    return msg\n",
    "\n",
    "\n",
    "# Implement PyAutoGUI actions based on gestures\n",
    "def execute_action(action):\n",
    "    if action == \"Swipe Up\":\n",
    "        pyautogui.hotkey('volumeup')  # volume up\n",
    "    elif action == \"Swipe Down\":\n",
    "        pyautogui.hotkey('volumedown')  # volume down\n",
    "    elif action == \"Swipe Right\":\n",
    "#         pyautogui.hotkey('p')  # for previous song\n",
    "        pyautogui.hotkey('right')\n",
    "    elif action == \"Swipe Left\":\n",
    "#         pyautogui.hotkey('n')  # for next song\n",
    "        pyautogui.hotkey('left')\n",
    "    elif action == \"Enter\":\n",
    "        pyautogui.press('space')  # pause/play\n",
    "    elif action == \"Backspace\":\n",
    "        pyautogui.press('backspace')\n",
    "    elif action == \"Tab\":\n",
    "        pyautogui.press('tab')\n",
    "    elif action == \"Ctrl_A\":\n",
    "        pyautogui.hotkey('ctrl', 'a')  # select all\n",
    "        \n",
    "        \n",
    "    time.sleep(1.0) \n",
    "\n",
    "\n",
    "# Main code\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "msg = \"\"\n",
    "s_msg = \"\"\n",
    "c = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        cv2.putText(image, \"NO HANDS\", (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last D-Gesture : \" + msg, (3, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Last S-Gesture : \" + s_msg, (3, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "            if results.right_hand_landmarks and not results.left_hand_landmarks:\n",
    "                l4 = results.right_hand_landmarks.landmark[4]\n",
    "                l12 = results.right_hand_landmarks.landmark[12]\n",
    "                dis = eucal(l4, l12)\n",
    "                if dis < 0.10:\n",
    "                    pointer(dis, \"\")\n",
    "                    continue\n",
    "            t_msg = s_msg\n",
    "            s, keys, s_msg = static()\n",
    "            if s_msg == \"None\":\n",
    "                s_msg = t_msg\n",
    "            if s == 1:\n",
    "                continue\n",
    "            else:\n",
    "                msg = await fun1(keys, 1)\n",
    "            c = 0\n",
    "        if cv2.waitKey(10) & 0xff == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Stop the text-to-speech thread\n",
    "tts_queue.put(None)\n",
    "tts_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
